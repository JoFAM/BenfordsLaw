---
title: "Using Benford's law for fraud detection"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      fig.align='center',
                      out.width='100%',
                      fig.width=8,
                      fig.height=4,
                      warning = FALSE,
                      message = FALSE)
library(ggplot2)
library(dplyr)
library(patchwork)
library(flextable)
```

--------------------

## What is Benford's law really?

The astronomer Simon Newcomb noticed in 1881 that the first pages of a book with logarithmic values were worn out more than later pages. He saw this as an indication that any random number had a higher chance of starting with a 1 than with a higher digit. His calculations led to the conclusion that for any digit $D$, the probability $P(D)$ that any random number would start with that digit is given by:

$$
P(D) = \text{log}_{10}\left( \frac{D + 1}{D}\right)
$$

In 1938, Frank Benford noticed the same thing and checked this theoretical distribution against a wide variety of datasets. Many of those datasets seemed to follow this law rather well, although some deviated significantly. 

```{r}
# Read in some functions we need
#source("https://raw.githubusercontent.com/JoFAM/BenfordsLaw/main/benfordFunctions.R")
source("../benfordFunctions.R")
# Read in the data downloaded from worldbank
d <- read.csv("../data/exportData.csv",skip = 4)
# just get the numbers
z <- as.matrix(d[-(1:4)])
z <- z[!is.na(z)]
```

As an example, I've downloaded the export data of all countries for the period 1960 to 2020 from the Worldbank website. This dataset contains in total `r length(z)` numbers. In the graphs below I show the expected distribution of the first digit according to Benford's law (left), and the observed distribution in this dataset (right). Both graphs look almost identical.

```{r}
df1 <- data.frame(prop = dBenf(1:9),
                  digit = as.character(1:9))
p1 <- ggplot(df1, aes(x = digit)) +
  geom_col(aes(y=prop)) +
  stat_function(aes(group = 1),
                fun = ~ log10(1 + 1/.)) +
  labs(y = "proportion",
       title = "Expected distribution of digits\naccording to Benford's law")

df2 <- fdigit(z) %>%
  table() %>%
  prop.table() %>%
  as.data.frame()
names(df2) <- c("digit","prop")
p2 <- ggplot(df2, aes(x = digit)) +
  geom_col(aes(y=prop)) +
  stat_function(aes(group = 1),
                fun = ~ log10(1 + 1/.)) +
  labs(y = "proportion",
       title = "Observed distribution of digits\n in export data.")

p1 + p2
```

```{r, echo = FALSE}
alldf <- left_join(df1, df2, by = "digit") %>%
  select(digit, 
         Expected = prop.x, 
         Observed = prop.y) %>%
  mutate(across(where(is.numeric),
                ~round(.,3)))
x <- as.data.frame(t(alldf[2:3]))
names(x) <- alldf[[1]]
x <- cbind(" " = rownames(x),
           x)
flextable::flextable(x)

```

Fast forward to 1992. A South-African PhD student, Mark J. Nigrini, finished his dissertation on using Benford's law to detect tax evasion. Although Benford's law was proposed before to detect fraud, it was the work of Nigrini that likely led to the popularisation of the concept. More importantly, it provided some guidelines to decide when deviation from the expected distribution was large enough to raise suspicion.

<div class="jumbotron">
  <b>Benford's law is no real law but an approximation/idealisation of the real world. Many datasets agree largely with Benford's law, but no real life dataset follows Benford's law perfectly.</b> </br></br>
  To use this for fraud detection, you need a way to determine:
  <ul>
  <li>whether a dataset is expected to follow the theoretical distribution, and if so,</li>
  <li>how much deviation is expected due to randomness</li>
  </ul>
</div>

## What does Benford's law imply about the data?

It's surprising we can find so many examples that largely follow Benford's law, because this law makes a very strong assumption. It expects that the logarithm of all numbers is equally likely to occur *within a given order of magnitude*. 

To clarify what this means, we'll use scientific notation. For example, the number $324$ can be written as $3.24 \times 10^2$. We call $3.24$ the mantissa and $2$ the exponent. Benford's law implies that the logarithm of the mantissa is uniformly distributed. This can be illustrated using the export dataset, as shown in the plot below.

```{r}
mydf <- data.frame(mantisse = z / 10^floor(log10(z)))
p <- ggplot(mydf, aes(x = mantisse)) +
  geom_histogram(bins = 10, boundary = 1,
                 color = "black") 
(p +
  scale_x_log10(limits = c(1,10)) +
  labs(title = "Logarithmic scale")|
  p + coord_cartesian(xlim = c(1,10)) +
  labs(title = "Original scale")) +
  plot_annotation(title = "Illustration of distribution of the mantissa using export data")
```

Any dataset that somehow violates this assumption, won't follow Benford's law. Some reasons for deviation are so obvious, that they lead to a set of general guidelines about which data might or might not fulfill the requirements. 

First of all, you expect a deviation when a dataset doesn't span an entire order of magnitude. For example, length of humans can't follow Benford's law. The average length of a newborn baby is 50cm, so in the order of magnitude $10^1$ to $10^2$ cm, you exclude already a huge part of possible values. But more importantly, in the order of magnitude $10^2$ to $10^3$cm, the only possible first digits are 1 and 2. Nobody is 3 meter or larger.

Second, you expect a deviation when a previous datapoint influences the value of the next one. For example, if you see 35,000 dollar on a personal saving account, the next value is likely going to be either a bit less or a bit more, but still in the range between 30,000 and 40,000. So also in this case the assumption won't hold.

Lastly, you expect a deviation simply due to randomness if you have a small dataset. We explore this further in the next section.

<div class="jumbotron">
<b>Unfortunately, there's no strict rule as to what data is expected to follow Benford's law, despite many attempts to formalise this.</b></br></br> 
As a general rule, a dataset is more likely to follow Benford if:
<ul>
 <li>it spans multiple orders of magnitude (the more, the merrier)</li>
 <li>the data is random (completely unrelated to eachother)</li>
 <li>the dataset is large (thousands of numbers is better than hundreds)</li>
</ul>
</div>

## How much deviation is too much?

### Expected range of values

As explained before, the amount of expected variation is largely dependent upon the amount of data. With smaller datasets you expect larger amounts of variation. But how much would one expect? One way to look at it, is to calculate between which boundaries the proportions will lie in 95% of the cases. 

According to Lesperance et al(2016), you can calculate these boundaries rather well using the method of Goodman. In the plot below, I compare these calculated confidence intervals with confidence intervals simulated based on a random sample from the export data. Both approaches are in rather good agreement, making Goodman a good alternative to computation-heavy simulations.

```{r}
alldf <- read.csv("../data/simulci_300.csv",
                  colClasses = c("character",
                                 rep("numeric",3)))
p1 <- ggplot(alldf, aes(x = digit)) +
  geom_crossbar(aes(y = prop,
                    ymin = ll,
                    ymax = ul,
                    fill = obs),
                position = position_dodge2(width = 0.4, padding = 0)) +
  labs(fill = "Confidence\ninterval",
       y = "proportion",
       title = "Sample of 300 observations")

alldf <- read.csv("../data/simulci_1000.csv",
                  colClasses = c("character",
                                 rep("numeric",3)))
p2 <- ggplot(alldf, aes(x = digit)) +
  geom_crossbar(aes(y = prop,
                    ymin = ll,
                    ymax = ul,
                    fill = obs),
                position = position_dodge2(width = 0.4, padding = 0)) +
  labs(fill = "Confidence\ninterval",
       y = "proportion",
       title = "Sample of 1,000 observations")

p1 + p2 +
  plot_layout(guides = "collect") +
  plot_annotation("Comparison of 95% confidence intervals")
```

### Negrini's MAD rule of thumb

In his work, Negrini recommends the use of the Mean Absolute Deviation (MAD) to judge whether something suspicious is going on in the data. You calculate the absolute difference between the expected and the observed proportion, and then take the average of that. To decide whether a dataset shows conformity with Benford's law, Nigrini suggested a treshold value of $0.012$; anything larger than that needs further investigation.

You can also look at this from a probabilistic angle. You can simulate all possible values for the MAD of a dataset with a given size. Then you check which value is larger than 95% of the simulated values. This is the 95% quantile. A sample from data that follows Benford's law will give a MAD larger than this quantile in only 5% of the cases. 

Doing this, you find a neat relation between the MAD and the sample size:

$$
MAD \sqrt{n} \approx 0.35 ~~~\text{with}~~~ MAD = \displaystyle \frac{1}{9} \sum_{i=1}^{9} \left|p_i - \frac{f_i}{n}\right|
$$

Here $p_i$ is the expected proportion for the $i$th digit, $f_i$ the observed frequency for that digit and $n$ the size of the dataset. Below you see a simulation table for different values of $n$. Values are rounded to 2 significant digits.

```{r}
resdf <- read.csv("../data/MADlimits.csv")
names(resdf) <- gsub("V","n = ",names(resdf))
resdf <- cbind(" " = c("Average","95% quantile","Rescaled Avg", "Rescaled quantile"),
               resdf)
flextable(resdf)
```

### Formal statistical testing

Whereas I determined the MAD treshold by simulation, often a more formal statistical approach is used. The most popular option is to use a Chi-squared test. Simply put, for every digit $i$ the squared difference is calculated between the observed frequencies $Fo_i$ and expected frequencies $Fe_i$. These numbers are divided by the expected frequency for that digit and the results are then summed. This gives the $\chi^2$ (read: chi squared) statistic:

$$
\chi^2 = \displaystyle \sum_{i=1}^9 \frac{\left( Fo_i - Fe_i \right)^2}{Fe_i}
$$

The value of this $\chi^2$ obviously grows larger when the data deviates further from the Benford optimum. Just like with the MAD, it's possible to calculate a treshold value that's only exceeded in 5% of the cases where the data does follow Benford's law. Because this is a formal statistical test, you can also calculate a so-called *p-value*. This value indicates in which fraction of the cases you'd expect a larger value for $\chi^2$ even though the sample comes from a dataset that follows Benford's law.

Just like the MAD, also this procedure is based on the differences between the observed and expected proportions. But now these differences are squared. As a consequence, the digit with the largest deviation will have a larger impact on the result of this test compared to the procedure based on the MAD.

<div class="jumbotron">
<b>You can't prove with certainty whether or not a specific dataset follows Benford's law. You can only get an indication of how likely certain values are assuming your data does follow Benford's law.</b></br></br> 
There's multiple approaches to do this:
<ul>
 <li>using boundaries within which the proportions of all digits lie in 95% of the cases.</li>
 <li>calculating the median absolute deviation and comparing to a treshold value.</li>
 <li>using formal statistics to get an idea of how extreme your data is if you assume it does follow Benford's law. </li>
</ul>
</div>

## References

### Background articles

 - [Hill, 1998](https://www.jstor.org/stable/27857060): The first digit phenomenon. (Can be downloaded from [this link](https://hill.math.gatech.edu/publications/PAPER%20PDFS/TheFirstDigitPhenomenonAmericanScientist1996.pdf) per 30 dec, 2021)
 - [Negrini, 2000](https://www.sciencedirect.com/science/article/pii/S0748575100000087): Computer assisted analytical procedures using Benford's Law
 - [Berger & Hill, 2011](https://projecteuclid.org/journals/probability-surveys/volume-8/issue-none/A-basic-theory-of-Benfords-Law/10.1214/11-PS175.full): A basic theory of Benford's law.
 - [Lesperance et al, 2016](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0151235#pone.0151235.ref021): Assessing conformance with Benford's law: goodness-of-fit tests and simultaneous confidence intervals.
 
### Application of Benford on COVID data

 - [Kennedy & Yam, 2020](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0243123): On the authenticity of COVID-19 case figures.
 
 
 
### Software 

 - Github repo for the Benford Analysis package [benford.analysis](https://github.com/carloscinelli/benford.analysis). This package is on CRAN.
 - Webpage for the [DescTools](https://andrisignorell.github.io/DescTools/) package that contains a set of functions to calculate probabilities and simulate datasets according to Benford's distribution. This package is on CRAN.
 
### Data

 - [Export data from worldbank.org](https://data.worldbank.org/indicator/NE.EXP.GNFS.KN?end=2020&start=1960): Indicator NE.EXP.GNFS.KN from 1960 to 2020. 